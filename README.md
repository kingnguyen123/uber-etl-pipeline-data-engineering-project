# Batch Processing Uber Data Analytics | Data Engineering AWS Project


## Introduction

>This project focuses on performing data analytics on Uber trip data using various tools and technologies on Amazon Web Services(AWS). The project demonstrates the use of S3 Bucket, Python, EC2, Apache Airflow, Redshift, and Power BI for an end-to-end data engineering and analytics workflow. 

## Architecture
The architecture of the project is as follows:
![Architecture diagramming](https://github.com/kingnguyen123/uber-etl-pipeline-data-engineering-project/blob/main/Architecture.png)

## Technology Used

- **Programming Language**: Python
- **Amazon Web Services(AWS)**:
  - S3 Bucket
  - EC2
  - Redshift
- **Batch workflow orchestration**: [Airflow](https://airflow.apache.org/)
- Power BI


## Dataset Used

The dataset used in this project is the TLC Trip Record Data.

- **Website**: [NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- 
>Data Dictionary - https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf

## Data Model
![Data Model](https://raw.githubusercontent.com/kingnguyen123/uber-etl-pipeline-data-engineering-project/main/data_model.jpeg)

## Data Analytics
![Data Analytics](https://github.com/kingnguyen123/uber-etl-pipeline-data-engineering-project/blob/main/Data%20Analytics.png)


## Conclusion

This project provided a comprehensive experience in building robust and scalable data engineering solutions using Amazon Web Services(AWS) and modern data tools. By integrating S3 Bucket, Python, EC2, Apache Airflow, Redshift, and Power BI, I was able to handle and analyze large datasets effectively.

Thank you for exploring this project. If you have any questions or suggestions, please feel free to reach out or contribute to the repository.





