from airflow import DAG
from datetime import datetime, timedelta
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd
from airflow.providers.postgres.hooks.postgres import PostgresHook
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 9, 19),
    'retries': 2,
    'retry_delay': timedelta(seconds=15)
}

def download_from_s3(bucket_name, key, local_path):
    s3_hook = S3Hook(aws_conn_id='my_s3_conn')
    s3_hook.get_key(key, bucket_name).download_file(local_path)

def transform_data(local_path, transformed_path):
    # Assuming the data is CSV for this example
    df = pd.read_csv(local_path)

    # Convert pickup and dropoff datetime to proper datetime format
    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])
    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])

    # Drop duplicates
    df = df.drop_duplicates().reset_index(drop=True)

    # Create datetime dimension
    datetime_dim = df[['tpep_pickup_datetime', 'tpep_dropoff_datetime']].reset_index(drop=True)
    datetime_dim['pick_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour
    datetime_dim['pick_day'] = datetime_dim['tpep_pickup_datetime'].dt.day
    datetime_dim['pick_month'] = datetime_dim['tpep_pickup_datetime'].dt.month
    datetime_dim['pick_year'] = datetime_dim['tpep_pickup_datetime'].dt.year
    datetime_dim['pick_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday

    datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour
    datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day
    datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month
    datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year
    datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday
    datetime_dim['datetime_id'] = datetime_dim.index

    datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year', 'pick_weekday',
                                 'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']]

    # Create other dimensions
    pickup_location_dim = df[['pickup_latitude', 'pickup_longitude']].reset_index(drop=True)
    pickup_location_dim['pickup_location_id'] = pickup_location_dim.index
    pickup_location_dim = pickup_location_dim[['pickup_location_id', 'pickup_latitude', 'pickup_longitude']]

    dropoff_location_dim = df[['dropoff_latitude', 'dropoff_longitude']].reset_index(drop=True)
    dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index
    dropoff_location_dim = dropoff_location_dim[['dropoff_location_id', 'dropoff_latitude', 'dropoff_longitude']]

    passenger_count_dim = df[['passenger_count']].reset_index(drop=True)
    passenger_count_dim['passenger_count_id'] = passenger_count_dim.index
    passenger_count_dim = passenger_count_dim[['passenger_count_id', 'passenger_count']]

    trip_distance_dim = df[['trip_distance']].reset_index(drop=True)
    trip_distance_dim['trip_distance_id'] = trip_distance_dim.index
    trip_distance_dim = trip_distance_dim[['trip_distance_id', 'trip_distance']]

    rate_code_type = {
        1: "Standard rate",
        2: "JFK",
        3: "Newark",
        4: "Nassau or Westchester",
        5: "Negotiated fare",
        6: "Group ride"
    }

    rate_code_dim = df[['RatecodeID']].reset_index(drop=True)
    rate_code_dim['rate_code_id'] = rate_code_dim.index
    rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type)
    rate_code_dim = rate_code_dim[['rate_code_id', 'RatecodeID', 'rate_code_name']]

    payment_type_name = {
        1: "Credit card",
        2: "Cash",
        3: "No charge",
        4: "Dispute",
        5: "Unknown",
        6: "Voided trip"
    }

    payment_type_dim = df[['payment_type']].reset_index(drop=True)
    payment_type_dim['payment_type_id'] = payment_type_dim.index
    payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name)
    payment_type_dim = payment_type_dim[['payment_type_id', 'payment_type', 'payment_type_name']]

    # Ensure trip_id exists (generate one if it doesn't)
    if 'trip_id' not in df.columns:
        df['trip_id'] = df.index

    # Create fact table by merging dimensions
    fact_table = df.merge(passenger_count_dim, left_on='trip_id', right_on='passenger_count_id') \
                   .merge(trip_distance_dim, left_on='trip_id', right_on='trip_distance_id') \
                   .merge(rate_code_dim, left_on='trip_id', right_on='rate_code_id') \
                   .merge(pickup_location_dim, left_on='trip_id', right_on='pickup_location_id') \
                   .merge(dropoff_location_dim, left_on='trip_id', right_on='dropoff_location_id') \
                   .merge(datetime_dim, left_on='trip_id', right_on='datetime_id') \
                   .merge(payment_type_dim, left_on='trip_id', right_on='payment_type_id') \
                   [['trip_id', 'VendorID', 'datetime_id', 'passenger_count_id', 'trip_distance_id', 'rate_code_id',
                     'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id', 'payment_type_id', 'fare_amount',
                     'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']]

    # Save the transformed data
    fact_table.to_csv(transformed_path, index=False)

def upload_to_s3(bucket_name, transformed_path, key):
    s3_hook = S3Hook(aws_conn_id='my_s3_conn')
    s3_hook.load_file(transformed_path, key, bucket_name)




def extract_from_redshift(redshift_conn_id, query, s3_bucket, s3_key):
    redshift_hook = PostgresHook(postgres_conn_id=redshift_conn_id)
    df = redshift_hook.get_pandas_df(sql=query)
    csv_buffer = df.to_csv(index=False)
    s3_hook = S3Hook(aws_conn_id='king')
    s3_hook.load_string(
        string_data=csv_buffer,
        key=s3_key,
        bucket_name=s3_bucket
    )



with DAG(dag_id='s3_transform_dag',
         default_args=default_args,
         schedule_interval='@daily',
         catchup=False) as dag:

    download_task = PythonOperator(
        task_id='download_from_s3',
        python_callable=download_from_s3,
        op_kwargs={'bucket_name': 'raw-uber-project-data',
                   'key': 'uber_data.csv',
                   'local_path': '/home/ubuntu/airflow/files/uber_data.csv'}
    )

    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        op_kwargs={'local_path': '/home/ubuntu/airflow/uber_data.csv',
                   'transformed_path': '/home/ubuntu/airflow/files/transformed_data.csv'}
    )

    upload_task = PythonOperator(
        task_id='upload_to_s3',
        python_callable=upload_to_s3,
        op_kwargs={'bucket_name': 'learn_data_uber',
                   'transformed_path': '/home/ubuntu/airflow/files/transformed_data.csv',
                   'key': 'transformed_data.csv'}
    )

    extract_task = PythonOperator(
        task_id='extract_from_redshift',
        python_callable=extract_from_redshift,
        op_kwargs={
            'redshift_conn_id': 'my_redshift_uber',  # Connection to Redshift
            'query': 'SELECT * FROM fact_table;',  # Your SQL query to extract data
            's3_bucket': 'learn_data_uber',
            's3_key': 'uber_data1.csv'
        }
    )
    download_task >> transform_task >> upload_task>>extract_task





